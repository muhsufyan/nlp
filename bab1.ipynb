{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLcZRarvV2L6rq4ooaUn3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhsufyan/nlp/blob/master/bab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWpLPlf-wLXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f5d5095f-5f16-42cb-8403-118f4dc13ea9"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qExYZ96TwU9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f54b19c-9a0a-4de6-c7ba-bfa5e0985884"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K83sH3gzwlEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5ebe2165-0190-40a1-bf52-f32c37f5c608"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "from nltk.book import *\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5EzdtDFxgBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1175e62f-34da-44c6-e40a-93478f8fb802"
      },
      "source": [
        "list = [text1,text2]\n",
        "print(list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Text: Moby Dick by Herman Melville 1851>, <Text: Sense and Sensibility by Jane Austen 1811>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDiN6-NsyMWm",
        "colab_type": "text"
      },
      "source": [
        "searching text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKsd1WpUyPnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "df4ab61c-5d46-4d80-a8b3-0278ab1926b9"
      },
      "source": [
        "text1.concordance(\"monstrous\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 11 of 11 matches:\n",
            "ong the former , one was of a most monstrous size . ... This came towards us , \n",
            "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
            "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
            "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
            "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
            "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
            "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
            "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
            "ere to enter upon those still more monstrous stories of them which are to be fo\n",
            "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
            "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tn8ZqYOysHb",
        "colab_type": "text"
      },
      "source": [
        "kemiripan\n",
        "concordance melihat kata secara konteks. For example, we saw that monstrous occurred in contexts such as the ___ pictures and a ___ size . What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkSO9BjDyujN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a11d9fb7-f45c-4c52-e109-ed7f57c2faf6"
      },
      "source": [
        "text1.similar(\"monstrous\")\n",
        "print(\"=\"*100)\n",
        "text2.similar(\"monstrous\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true contemptible christian abundant few part mean careful puzzled\n",
            "mystifying passing curious loving wise doleful gamesome singular\n",
            "delightfully perilous fearless\n",
            "====================================================================================================\n",
            "very so exceedingly heartily a as good great extremely remarkably\n",
            "sweet vast amazingly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieD2j8tzYCT",
        "colab_type": "text"
      },
      "source": [
        "Austen uses this word quite differently from Melville; for her, monstrous has positive connotations, and sometimes functions as an intensifier like the word very.\n",
        "The term *common_contexts* allows us to examine just the contexts that are shared by two or more words, such as monstrous and very"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXnyDIiCziXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35619676-fb8d-45d2-c73b-08feaeb9e4a1"
      },
      "source": [
        " text2.common_contexts([\"monstrous\", \"very\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_pretty am_glad a_lucky is_pretty be_glad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rqCjSyd1JUY",
        "colab_type": "text"
      },
      "source": [
        "we can also determine the *location* of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a **dispersion plot**. Each stripe represents an instance of a word, and each row represents the entire text.\n",
        "\n",
        "You can produce this plot as shown below. You might like to try more words (e.g., liberty, constitution), and different texts. Can you predict the dispersion of a word before you view it? \n",
        "\n",
        "kurang mengerti mengenai fungsi dispersion_plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8ohvItS1ZHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "032da12b-4b22-48ff-cd4b-28a0aea6854d"
      },
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNbQ1_up3eBU",
        "colab_type": "text"
      },
      "source": [
        " let's try generating some random text in the various styles we have just seen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh42SyMM3f9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "8735e4f3-e04c-4055-ad0b-111244898d70"
      },
      "source": [
        "text3.generate()\n",
        "# error tdk tau kenapa"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f5ed39cee8ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# error tdk tau kenapa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: generate() missing 1 required positional argument: 'words'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukGIu5-t48JM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f2354b5-5602-40bd-cf99-357c3dc19ef5"
      },
      "source": [
        "print('The nltk version is {}.'.format(nltk.__version__))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nltk version is 3.2.5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNm_jCOF5Jpx",
        "colab_type": "text"
      },
      "source": [
        "finding out the length of a text from start to finish, in terms of the words and punctuation symbols that appear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OIJmNqo5KTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a67067cf-3b4f-4cd1-b056-607a241e9f07"
      },
      "source": [
        "len(text3)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_xqxNBc5SeB",
        "colab_type": "text"
      },
      "source": [
        "So Genesis has 44,764 words and punctuation symbols, or \"tokens.\" A token is the technical name for a sequence of characters — such as hairy, his, or :) — that we want to treat as a group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fXOEPzX5n8h",
        "colab_type": "text"
      },
      "source": [
        "When we count the number of tokens in a text, say, the phrase to be or not to be, we are counting occurrences of these sequences. Thus, in our example phrase there are two occurrences of to, two of be, and one each of or and not. But there are only four distinct vocabulary items in this phrase. How many distinct words does the book of Genesis contain? To work this out in Python, we have to pose the question slightly differently. The vocabulary of a text is just the set of tokens that it uses, since in a set, all duplicates are collapsed together. In Python we can obtain the vocabulary items of text3 with the command: set(text3). When you do this, many screens of words will fly past. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiBdvscK5piA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4054eb0-048f-4614-900e-2d0c874d8ee7"
      },
      "source": [
        "sorted(set(text3))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " ',)',\n",
              " '.',\n",
              " '.)',\n",
              " ':',\n",
              " ';',\n",
              " ';)',\n",
              " '?',\n",
              " '?)',\n",
              " 'A',\n",
              " 'Abel',\n",
              " 'Abelmizraim',\n",
              " 'Abidah',\n",
              " 'Abide',\n",
              " 'Abimael',\n",
              " 'Abimelech',\n",
              " 'Abr',\n",
              " 'Abrah',\n",
              " 'Abraham',\n",
              " 'Abram',\n",
              " 'Accad',\n",
              " 'Achbor',\n",
              " 'Adah',\n",
              " 'Adam',\n",
              " 'Adbeel',\n",
              " 'Admah',\n",
              " 'Adullamite',\n",
              " 'After',\n",
              " 'Aholibamah',\n",
              " 'Ahuzzath',\n",
              " 'Ajah',\n",
              " 'Akan',\n",
              " 'All',\n",
              " 'Allonbachuth',\n",
              " 'Almighty',\n",
              " 'Almodad',\n",
              " 'Also',\n",
              " 'Alvah',\n",
              " 'Alvan',\n",
              " 'Am',\n",
              " 'Amal',\n",
              " 'Amalek',\n",
              " 'Amalekites',\n",
              " 'Ammon',\n",
              " 'Amorite',\n",
              " 'Amorites',\n",
              " 'Amraphel',\n",
              " 'An',\n",
              " 'Anah',\n",
              " 'Anamim',\n",
              " 'And',\n",
              " 'Aner',\n",
              " 'Angel',\n",
              " 'Appoint',\n",
              " 'Aram',\n",
              " 'Aran',\n",
              " 'Ararat',\n",
              " 'Arbah',\n",
              " 'Ard',\n",
              " 'Are',\n",
              " 'Areli',\n",
              " 'Arioch',\n",
              " 'Arise',\n",
              " 'Arkite',\n",
              " 'Arodi',\n",
              " 'Arphaxad',\n",
              " 'Art',\n",
              " 'Arvadite',\n",
              " 'As',\n",
              " 'Asenath',\n",
              " 'Ashbel',\n",
              " 'Asher',\n",
              " 'Ashkenaz',\n",
              " 'Ashteroth',\n",
              " 'Ask',\n",
              " 'Asshur',\n",
              " 'Asshurim',\n",
              " 'Assyr',\n",
              " 'Assyria',\n",
              " 'At',\n",
              " 'Atad',\n",
              " 'Avith',\n",
              " 'Baalhanan',\n",
              " 'Babel',\n",
              " 'Bashemath',\n",
              " 'Be',\n",
              " 'Because',\n",
              " 'Becher',\n",
              " 'Bedad',\n",
              " 'Beeri',\n",
              " 'Beerlahairoi',\n",
              " 'Beersheba',\n",
              " 'Behold',\n",
              " 'Bela',\n",
              " 'Belah',\n",
              " 'Benam',\n",
              " 'Benjamin',\n",
              " 'Beno',\n",
              " 'Beor',\n",
              " 'Bera',\n",
              " 'Bered',\n",
              " 'Beriah',\n",
              " 'Bethel',\n",
              " 'Bethlehem',\n",
              " 'Bethuel',\n",
              " 'Beware',\n",
              " 'Bilhah',\n",
              " 'Bilhan',\n",
              " 'Binding',\n",
              " 'Birsha',\n",
              " 'Bless',\n",
              " 'Blessed',\n",
              " 'Both',\n",
              " 'Bow',\n",
              " 'Bozrah',\n",
              " 'Bring',\n",
              " 'But',\n",
              " 'Buz',\n",
              " 'By',\n",
              " 'Cain',\n",
              " 'Cainan',\n",
              " 'Calah',\n",
              " 'Calneh',\n",
              " 'Can',\n",
              " 'Cana',\n",
              " 'Canaan',\n",
              " 'Canaanite',\n",
              " 'Canaanites',\n",
              " 'Canaanitish',\n",
              " 'Caphtorim',\n",
              " 'Carmi',\n",
              " 'Casluhim',\n",
              " 'Cast',\n",
              " 'Cause',\n",
              " 'Chaldees',\n",
              " 'Chedorlaomer',\n",
              " 'Cheran',\n",
              " 'Cherubims',\n",
              " 'Chesed',\n",
              " 'Chezib',\n",
              " 'Come',\n",
              " 'Cursed',\n",
              " 'Cush',\n",
              " 'Damascus',\n",
              " 'Dan',\n",
              " 'Day',\n",
              " 'Deborah',\n",
              " 'Dedan',\n",
              " 'Deliver',\n",
              " 'Diklah',\n",
              " 'Din',\n",
              " 'Dinah',\n",
              " 'Dinhabah',\n",
              " 'Discern',\n",
              " 'Dishan',\n",
              " 'Dishon',\n",
              " 'Do',\n",
              " 'Dodanim',\n",
              " 'Dothan',\n",
              " 'Drink',\n",
              " 'Duke',\n",
              " 'Dumah',\n",
              " 'Earth',\n",
              " 'Ebal',\n",
              " 'Eber',\n",
              " 'Edar',\n",
              " 'Eden',\n",
              " 'Edom',\n",
              " 'Edomites',\n",
              " 'Egy',\n",
              " 'Egypt',\n",
              " 'Egyptia',\n",
              " 'Egyptian',\n",
              " 'Egyptians',\n",
              " 'Ehi',\n",
              " 'Elah',\n",
              " 'Elam',\n",
              " 'Elbethel',\n",
              " 'Eldaah',\n",
              " 'EleloheIsrael',\n",
              " 'Eliezer',\n",
              " 'Eliphaz',\n",
              " 'Elishah',\n",
              " 'Ellasar',\n",
              " 'Elon',\n",
              " 'Elparan',\n",
              " 'Emins',\n",
              " 'En',\n",
              " 'Enmishpat',\n",
              " 'Eno',\n",
              " 'Enoch',\n",
              " 'Enos',\n",
              " 'Ephah',\n",
              " 'Epher',\n",
              " 'Ephra',\n",
              " 'Ephraim',\n",
              " 'Ephrath',\n",
              " 'Ephron',\n",
              " 'Er',\n",
              " 'Erech',\n",
              " 'Eri',\n",
              " 'Es',\n",
              " 'Esau',\n",
              " 'Escape',\n",
              " 'Esek',\n",
              " 'Eshban',\n",
              " 'Eshcol',\n",
              " 'Ethiopia',\n",
              " 'Euphrat',\n",
              " 'Euphrates',\n",
              " 'Eve',\n",
              " 'Even',\n",
              " 'Every',\n",
              " 'Except',\n",
              " 'Ezbon',\n",
              " 'Ezer',\n",
              " 'Fear',\n",
              " 'Feed',\n",
              " 'Fifteen',\n",
              " 'Fill',\n",
              " 'For',\n",
              " 'Forasmuch',\n",
              " 'Forgive',\n",
              " 'From',\n",
              " 'Fulfil',\n",
              " 'G',\n",
              " 'Gad',\n",
              " 'Gaham',\n",
              " 'Galeed',\n",
              " 'Gatam',\n",
              " 'Gather',\n",
              " 'Gaza',\n",
              " 'Gentiles',\n",
              " 'Gera',\n",
              " 'Gerar',\n",
              " 'Gershon',\n",
              " 'Get',\n",
              " 'Gether',\n",
              " 'Gihon',\n",
              " 'Gilead',\n",
              " 'Girgashites',\n",
              " 'Girgasite',\n",
              " 'Give',\n",
              " 'Go',\n",
              " 'God',\n",
              " 'Gomer',\n",
              " 'Gomorrah',\n",
              " 'Goshen',\n",
              " 'Guni',\n",
              " 'Hadad',\n",
              " 'Hadar',\n",
              " 'Hadoram',\n",
              " 'Hagar',\n",
              " 'Haggi',\n",
              " 'Hai',\n",
              " 'Ham',\n",
              " 'Hamathite',\n",
              " 'Hamor',\n",
              " 'Hamul',\n",
              " 'Hanoch',\n",
              " 'Happy',\n",
              " 'Haran',\n",
              " 'Hast',\n",
              " 'Haste',\n",
              " 'Have',\n",
              " 'Havilah',\n",
              " 'Hazarmaveth',\n",
              " 'Hazezontamar',\n",
              " 'Hazo',\n",
              " 'He',\n",
              " 'Hear',\n",
              " 'Heaven',\n",
              " 'Heber',\n",
              " 'Hebrew',\n",
              " 'Hebrews',\n",
              " 'Hebron',\n",
              " 'Hemam',\n",
              " 'Hemdan',\n",
              " 'Here',\n",
              " 'Hereby',\n",
              " 'Heth',\n",
              " 'Hezron',\n",
              " 'Hiddekel',\n",
              " 'Hinder',\n",
              " 'Hirah',\n",
              " 'His',\n",
              " 'Hitti',\n",
              " 'Hittite',\n",
              " 'Hittites',\n",
              " 'Hivite',\n",
              " 'Hobah',\n",
              " 'Hori',\n",
              " 'Horite',\n",
              " 'Horites',\n",
              " 'How',\n",
              " 'Hul',\n",
              " 'Huppim',\n",
              " 'Husham',\n",
              " 'Hushim',\n",
              " 'Huz',\n",
              " 'I',\n",
              " 'If',\n",
              " 'In',\n",
              " 'Irad',\n",
              " 'Iram',\n",
              " 'Is',\n",
              " 'Isa',\n",
              " 'Isaac',\n",
              " 'Iscah',\n",
              " 'Ishbak',\n",
              " 'Ishmael',\n",
              " 'Ishmeelites',\n",
              " 'Ishuah',\n",
              " 'Isra',\n",
              " 'Israel',\n",
              " 'Issachar',\n",
              " 'Isui',\n",
              " 'It',\n",
              " 'Ithran',\n",
              " 'Jaalam',\n",
              " 'Jabal',\n",
              " 'Jabbok',\n",
              " 'Jac',\n",
              " 'Jachin',\n",
              " 'Jacob',\n",
              " 'Jahleel',\n",
              " 'Jahzeel',\n",
              " 'Jamin',\n",
              " 'Japhe',\n",
              " 'Japheth',\n",
              " 'Jared',\n",
              " 'Javan',\n",
              " 'Jebusite',\n",
              " 'Jebusites',\n",
              " 'Jegarsahadutha',\n",
              " 'Jehovahjireh',\n",
              " 'Jemuel',\n",
              " 'Jerah',\n",
              " 'Jetheth',\n",
              " 'Jetur',\n",
              " 'Jeush',\n",
              " 'Jezer',\n",
              " 'Jidlaph',\n",
              " 'Jimnah',\n",
              " 'Job',\n",
              " 'Jobab',\n",
              " 'Jokshan',\n",
              " 'Joktan',\n",
              " 'Jordan',\n",
              " 'Joseph',\n",
              " 'Jubal',\n",
              " 'Judah',\n",
              " 'Judge',\n",
              " 'Judith',\n",
              " 'Kadesh',\n",
              " 'Kadmonites',\n",
              " 'Karnaim',\n",
              " 'Kedar',\n",
              " 'Kedemah',\n",
              " 'Kemuel',\n",
              " 'Kenaz',\n",
              " 'Kenites',\n",
              " 'Kenizzites',\n",
              " 'Keturah',\n",
              " 'Kiriathaim',\n",
              " 'Kirjatharba',\n",
              " 'Kittim',\n",
              " 'Know',\n",
              " 'Kohath',\n",
              " 'Kor',\n",
              " 'Korah',\n",
              " 'LO',\n",
              " 'LORD',\n",
              " 'Laban',\n",
              " 'Lahairoi',\n",
              " 'Lamech',\n",
              " 'Lasha',\n",
              " 'Lay',\n",
              " 'Leah',\n",
              " 'Lehabim',\n",
              " 'Lest',\n",
              " 'Let',\n",
              " 'Letushim',\n",
              " 'Leummim',\n",
              " 'Levi',\n",
              " 'Lie',\n",
              " 'Lift',\n",
              " 'Lo',\n",
              " 'Look',\n",
              " 'Lot',\n",
              " 'Lotan',\n",
              " 'Lud',\n",
              " 'Ludim',\n",
              " 'Luz',\n",
              " 'Maachah',\n",
              " 'Machir',\n",
              " 'Machpelah',\n",
              " 'Madai',\n",
              " 'Magdiel',\n",
              " 'Magog',\n",
              " 'Mahalaleel',\n",
              " 'Mahalath',\n",
              " 'Mahanaim',\n",
              " 'Make',\n",
              " 'Malchiel',\n",
              " 'Male',\n",
              " 'Mam',\n",
              " 'Mamre',\n",
              " 'Man',\n",
              " 'Manahath',\n",
              " 'Manass',\n",
              " 'Manasseh',\n",
              " 'Mash',\n",
              " 'Masrekah',\n",
              " 'Massa',\n",
              " 'Matred',\n",
              " 'Me',\n",
              " 'Medan',\n",
              " 'Mehetabel',\n",
              " 'Mehujael',\n",
              " 'Melchizedek',\n",
              " 'Merari',\n",
              " 'Mesha',\n",
              " 'Meshech',\n",
              " 'Mesopotamia',\n",
              " 'Methusa',\n",
              " 'Methusael',\n",
              " 'Methuselah',\n",
              " 'Mezahab',\n",
              " 'Mibsam',\n",
              " 'Mibzar',\n",
              " 'Midian',\n",
              " 'Midianites',\n",
              " 'Milcah',\n",
              " 'Mishma',\n",
              " 'Mizpah',\n",
              " 'Mizraim',\n",
              " 'Mizz',\n",
              " 'Moab',\n",
              " 'Moabites',\n",
              " 'Moreh',\n",
              " 'Moreover',\n",
              " 'Moriah',\n",
              " 'Muppim',\n",
              " 'My',\n",
              " 'Naamah',\n",
              " 'Naaman',\n",
              " 'Nahath',\n",
              " 'Nahor',\n",
              " 'Naphish',\n",
              " 'Naphtali',\n",
              " 'Naphtuhim',\n",
              " 'Nay',\n",
              " 'Nebajoth',\n",
              " 'Neither',\n",
              " 'Night',\n",
              " 'Nimrod',\n",
              " 'Nineveh',\n",
              " 'Noah',\n",
              " 'Nod',\n",
              " 'Not',\n",
              " 'Now',\n",
              " 'O',\n",
              " 'Obal',\n",
              " 'Of',\n",
              " 'Oh',\n",
              " 'Ohad',\n",
              " 'Omar',\n",
              " 'On',\n",
              " 'Onam',\n",
              " 'Onan',\n",
              " 'Only',\n",
              " 'Ophir',\n",
              " 'Our',\n",
              " 'Out',\n",
              " 'Padan',\n",
              " 'Padanaram',\n",
              " 'Paran',\n",
              " 'Pass',\n",
              " 'Pathrusim',\n",
              " 'Pau',\n",
              " 'Peace',\n",
              " 'Peleg',\n",
              " 'Peniel',\n",
              " 'Penuel',\n",
              " 'Peradventure',\n",
              " 'Perizzit',\n",
              " 'Perizzite',\n",
              " 'Perizzites',\n",
              " 'Phallu',\n",
              " 'Phara',\n",
              " 'Pharaoh',\n",
              " 'Pharez',\n",
              " 'Phichol',\n",
              " 'Philistim',\n",
              " 'Philistines',\n",
              " 'Phut',\n",
              " 'Phuvah',\n",
              " 'Pildash',\n",
              " 'Pinon',\n",
              " 'Pison',\n",
              " 'Potiphar',\n",
              " 'Potipherah',\n",
              " 'Put',\n",
              " 'Raamah',\n",
              " 'Rachel',\n",
              " 'Rameses',\n",
              " 'Rebek',\n",
              " 'Rebekah',\n",
              " 'Rehoboth',\n",
              " 'Remain',\n",
              " 'Rephaims',\n",
              " 'Resen',\n",
              " 'Return',\n",
              " 'Reu',\n",
              " 'Reub',\n",
              " 'Reuben',\n",
              " 'Reuel',\n",
              " 'Reumah',\n",
              " 'Riphath',\n",
              " 'Rosh',\n",
              " 'Sabtah',\n",
              " 'Sabtech',\n",
              " 'Said',\n",
              " 'Salah',\n",
              " 'Salem',\n",
              " 'Samlah',\n",
              " 'Sarah',\n",
              " 'Sarai',\n",
              " 'Saul',\n",
              " 'Save',\n",
              " 'Say',\n",
              " 'Se',\n",
              " 'Seba',\n",
              " 'See',\n",
              " 'Seeing',\n",
              " 'Seir',\n",
              " 'Sell',\n",
              " 'Send',\n",
              " 'Sephar',\n",
              " 'Serah',\n",
              " 'Sered',\n",
              " 'Serug',\n",
              " 'Set',\n",
              " 'Seth',\n",
              " 'Shalem',\n",
              " 'Shall',\n",
              " 'Shalt',\n",
              " 'Shammah',\n",
              " 'Shaul',\n",
              " 'Shaveh',\n",
              " 'She',\n",
              " 'Sheba',\n",
              " 'Shebah',\n",
              " 'Shechem',\n",
              " 'Shed',\n",
              " 'Shel',\n",
              " 'Shelah',\n",
              " 'Sheleph',\n",
              " 'Shem',\n",
              " 'Shemeber',\n",
              " 'Shepho',\n",
              " 'Shillem',\n",
              " 'Shiloh',\n",
              " 'Shimron',\n",
              " 'Shinab',\n",
              " 'Shinar',\n",
              " 'Shobal',\n",
              " 'Should',\n",
              " 'Shuah',\n",
              " 'Shuni',\n",
              " 'Shur',\n",
              " 'Sichem',\n",
              " 'Siddim',\n",
              " 'Sidon',\n",
              " 'Simeon',\n",
              " 'Sinite',\n",
              " 'Sitnah',\n",
              " 'Slay',\n",
              " 'So',\n",
              " 'Sod',\n",
              " 'Sodom',\n",
              " 'Sojourn',\n",
              " 'Some',\n",
              " 'Spake',\n",
              " 'Speak',\n",
              " 'Spirit',\n",
              " 'Stand',\n",
              " 'Succoth',\n",
              " 'Surely',\n",
              " 'Swear',\n",
              " 'Syrian',\n",
              " 'Take',\n",
              " 'Tamar',\n",
              " 'Tarshish',\n",
              " 'Tebah',\n",
              " 'Tell',\n",
              " 'Tema',\n",
              " 'Teman',\n",
              " 'Temani',\n",
              " 'Terah',\n",
              " 'Thahash',\n",
              " 'That',\n",
              " 'The',\n",
              " 'Then',\n",
              " 'There',\n",
              " 'Therefore',\n",
              " 'These',\n",
              " 'They',\n",
              " 'Thirty',\n",
              " 'This',\n",
              " 'Thorns',\n",
              " 'Thou',\n",
              " 'Thus',\n",
              " 'Thy',\n",
              " 'Tidal',\n",
              " 'Timna',\n",
              " 'Timnah',\n",
              " 'Timnath',\n",
              " 'Tiras',\n",
              " 'To',\n",
              " 'Togarmah',\n",
              " 'Tola',\n",
              " 'Tubal',\n",
              " 'Tubalcain',\n",
              " 'Twelve',\n",
              " 'Two',\n",
              " 'Unstable',\n",
              " 'Until',\n",
              " 'Unto',\n",
              " 'Up',\n",
              " 'Upon',\n",
              " 'Ur',\n",
              " 'Uz',\n",
              " 'Uzal',\n",
              " 'We',\n",
              " 'What',\n",
              " 'When',\n",
              " 'Whence',\n",
              " 'Where',\n",
              " 'Whereas',\n",
              " 'Wherefore',\n",
              " 'Which',\n",
              " 'While',\n",
              " 'Who',\n",
              " 'Whose',\n",
              " 'Whoso',\n",
              " 'Why',\n",
              " 'Wilt',\n",
              " 'With',\n",
              " 'Woman',\n",
              " 'Ye',\n",
              " 'Yea',\n",
              " 'Yet',\n",
              " 'Zaavan',\n",
              " 'Zaphnathpaaneah',\n",
              " 'Zar',\n",
              " 'Zarah',\n",
              " 'Zeboiim',\n",
              " 'Zeboim',\n",
              " 'Zebul',\n",
              " 'Zebulun',\n",
              " 'Zemarite',\n",
              " 'Zepho',\n",
              " 'Zerah',\n",
              " 'Zibeon',\n",
              " 'Zidon',\n",
              " 'Zillah',\n",
              " 'Zilpah',\n",
              " 'Zimran',\n",
              " 'Ziphion',\n",
              " 'Zo',\n",
              " 'Zoar',\n",
              " 'Zohar',\n",
              " 'Zuzims',\n",
              " 'a',\n",
              " 'abated',\n",
              " 'abide',\n",
              " 'able',\n",
              " 'abode',\n",
              " 'abomination',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abroad',\n",
              " 'absent',\n",
              " 'abundantly',\n",
              " 'accept',\n",
              " 'accepted',\n",
              " 'according',\n",
              " 'acknowledged',\n",
              " 'activity',\n",
              " 'add',\n",
              " 'adder',\n",
              " 'afar',\n",
              " 'afflict',\n",
              " 'affliction',\n",
              " 'afraid',\n",
              " 'after',\n",
              " 'afterward',\n",
              " 'afterwards',\n",
              " 'aga',\n",
              " 'again',\n",
              " 'against',\n",
              " 'age',\n",
              " 'aileth',\n",
              " 'air',\n",
              " 'al',\n",
              " 'alive',\n",
              " 'all',\n",
              " 'almon',\n",
              " 'alo',\n",
              " 'alone',\n",
              " 'aloud',\n",
              " 'also',\n",
              " 'altar',\n",
              " 'altogether',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'an',\n",
              " 'and',\n",
              " 'angel',\n",
              " 'angels',\n",
              " 'anger',\n",
              " 'angry',\n",
              " 'anguish',\n",
              " 'anointedst',\n",
              " 'anoth',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'answered',\n",
              " 'any',\n",
              " 'anything',\n",
              " 'appe',\n",
              " 'appear',\n",
              " 'appeared',\n",
              " 'appease',\n",
              " 'appoint',\n",
              " 'appointed',\n",
              " 'aprons',\n",
              " 'archer',\n",
              " 'archers',\n",
              " 'are',\n",
              " 'arise',\n",
              " 'ark',\n",
              " 'armed',\n",
              " 'arms',\n",
              " 'army',\n",
              " 'arose',\n",
              " 'arrayed',\n",
              " 'art',\n",
              " 'artificer',\n",
              " 'as',\n",
              " 'ascending',\n",
              " 'ash',\n",
              " 'ashamed',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asketh',\n",
              " 'ass',\n",
              " 'assembly',\n",
              " 'asses',\n",
              " 'assigned',\n",
              " 'asswaged',\n",
              " 'at',\n",
              " 'attained',\n",
              " 'audience',\n",
              " 'avenged',\n",
              " 'aw',\n",
              " 'awaked',\n",
              " 'away',\n",
              " 'awoke',\n",
              " 'back',\n",
              " 'backward',\n",
              " 'bad',\n",
              " 'bade',\n",
              " 'badest',\n",
              " 'badne',\n",
              " 'bak',\n",
              " 'bake',\n",
              " 'bakemeats',\n",
              " 'baker',\n",
              " 'bakers',\n",
              " 'balm',\n",
              " 'bands',\n",
              " 'bank',\n",
              " 'bare',\n",
              " 'barr',\n",
              " 'barren',\n",
              " 'basket',\n",
              " 'baskets',\n",
              " 'battle',\n",
              " 'bdellium',\n",
              " 'be',\n",
              " 'bear',\n",
              " 'beari',\n",
              " 'bearing',\n",
              " 'beast',\n",
              " 'beasts',\n",
              " 'beautiful',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'bed',\n",
              " 'been',\n",
              " 'befall',\n",
              " 'befell',\n",
              " 'before',\n",
              " 'began',\n",
              " 'begat',\n",
              " 'beget',\n",
              " 'begettest',\n",
              " 'begin',\n",
              " 'beginning',\n",
              " 'begotten',\n",
              " 'beguiled',\n",
              " 'beheld',\n",
              " 'behind',\n",
              " 'behold',\n",
              " 'being',\n",
              " 'believed',\n",
              " 'belly',\n",
              " 'belong',\n",
              " 'beneath',\n",
              " 'bereaved',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'besought',\n",
              " 'best',\n",
              " 'betimes',\n",
              " 'better',\n",
              " 'between',\n",
              " 'betwixt',\n",
              " 'beyond',\n",
              " 'binding',\n",
              " 'bird',\n",
              " 'birds',\n",
              " 'birthday',\n",
              " 'birthright',\n",
              " 'biteth',\n",
              " 'bitter',\n",
              " 'blame',\n",
              " 'blameless',\n",
              " 'blasted',\n",
              " 'bless',\n",
              " 'blessed',\n",
              " 'blesseth',\n",
              " 'blessi',\n",
              " 'blessing',\n",
              " 'blessings',\n",
              " 'blindness',\n",
              " 'blood',\n",
              " 'blossoms',\n",
              " 'bodies',\n",
              " 'boldly',\n",
              " 'bondman',\n",
              " 'bondmen',\n",
              " 'bondwoman',\n",
              " 'bone',\n",
              " 'bones',\n",
              " 'book',\n",
              " 'booths',\n",
              " 'border',\n",
              " 'borders',\n",
              " 'born',\n",
              " 'bosom',\n",
              " 'both',\n",
              " 'bottle',\n",
              " 'bou',\n",
              " 'boug',\n",
              " 'bough',\n",
              " 'bought',\n",
              " 'bound',\n",
              " 'bow',\n",
              " 'bowed',\n",
              " 'bowels',\n",
              " 'bowing',\n",
              " 'boys',\n",
              " 'bracelets',\n",
              " 'branches',\n",
              " 'brass',\n",
              " 'bre',\n",
              " 'breach',\n",
              " 'bread',\n",
              " 'breadth',\n",
              " 'break',\n",
              " 'breaketh',\n",
              " 'breaking',\n",
              " 'breasts',\n",
              " 'breath',\n",
              " 'breathed',\n",
              " 'breed',\n",
              " 'brethren',\n",
              " 'brick',\n",
              " 'brimstone',\n",
              " 'bring',\n",
              " 'brink',\n",
              " 'broken',\n",
              " 'brook',\n",
              " 'broth',\n",
              " 'brother',\n",
              " 'brought',\n",
              " 'brown',\n",
              " 'bruise',\n",
              " 'budded',\n",
              " 'build',\n",
              " 'builded',\n",
              " 'built',\n",
              " 'bulls',\n",
              " 'bundle',\n",
              " 'bundles',\n",
              " 'burdens',\n",
              " 'buried',\n",
              " 'burn',\n",
              " 'burning',\n",
              " 'burnt',\n",
              " 'bury',\n",
              " 'buryingplace',\n",
              " 'business',\n",
              " 'but',\n",
              " 'butler',\n",
              " 'butlers',\n",
              " 'butlership',\n",
              " 'butter',\n",
              " 'buy',\n",
              " 'by',\n",
              " 'cakes',\n",
              " 'calf',\n",
              " 'call',\n",
              " 'called',\n",
              " 'came',\n",
              " 'camel',\n",
              " 'camels',\n",
              " 'camest',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'canst',\n",
              " 'captain',\n",
              " 'captive',\n",
              " 'captives',\n",
              " 'carcases',\n",
              " 'carried',\n",
              " 'carry',\n",
              " 'cast',\n",
              " 'castles',\n",
              " 'catt',\n",
              " 'cattle',\n",
              " 'caught',\n",
              " 'cause',\n",
              " 'caused',\n",
              " 'cave',\n",
              " 'cease',\n",
              " 'ceased',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'chain',\n",
              " 'chamber',\n",
              " 'change',\n",
              " 'changed',\n",
              " 'changes',\n",
              " 'charge',\n",
              " 'charged',\n",
              " 'chariot',\n",
              " 'chariots',\n",
              " 'chesnut',\n",
              " 'chi',\n",
              " 'chief',\n",
              " 'child',\n",
              " 'childless',\n",
              " 'childr',\n",
              " 'children',\n",
              " 'chode',\n",
              " 'choice',\n",
              " 'chose',\n",
              " 'circumcis',\n",
              " 'circumcise',\n",
              " 'circumcised',\n",
              " 'citi',\n",
              " 'cities',\n",
              " 'city',\n",
              " 'clave',\n",
              " 'clean',\n",
              " 'clear',\n",
              " 'cleave',\n",
              " 'clo',\n",
              " 'closed',\n",
              " 'clothed',\n",
              " 'clothes',\n",
              " 'cloud',\n",
              " 'clusters',\n",
              " 'co',\n",
              " 'coat',\n",
              " 'coats',\n",
              " 'coffin',\n",
              " 'cold',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pvwcJkQ6KWR",
        "colab_type": "text"
      },
      "source": [
        "By wrapping sorted() around the Python expression set(text3), we obtain a sorted list of vocabulary items, beginning with various punctuation symbols and continuing with words starting with A. All capitalized words precede lowercase words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyazHyeK6Azw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf51d1db-0b6e-4e45-ec7d-cc6a94a07d78"
      },
      "source": [
        "len(set(text3))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK5Hd3os6rqQ",
        "colab_type": "text"
      },
      "source": [
        "We discover the size of the vocabulary indirectly, by asking for the number of items in the set, and again we can use *len* to obtain this number. Although it has 44,764 tokens, this book has only 2,789 distinct words, or \"word types.\" A **word type** is the form or spelling of the word independently of its specific occurrences in a text — that is, the word considered as a unique item of vocabulary. Our count of 2,789 items will include punctuation symbols, so we will generally call these unique items types instead of word types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDd4gE-7EU-",
        "colab_type": "text"
      },
      "source": [
        "let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DiHVgum6_Km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee0ff332-f613-48ae-daa2-7d715f3bbace"
      },
      "source": [
        "len(set(text3)) / len(text3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06230453042623537"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Uv3S6y8Zss",
        "colab_type": "text"
      },
      "source": [
        " let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql_Nrw4Q8awU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d83d367f-4832-46ef-e5da-62dfedd72d54"
      },
      "source": [
        "text3.count(\"smote\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS0FXwUs8mWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97218973-b5c4-4bf4-f479-887550425f4a"
      },
      "source": [
        " 100 * text4.count('a') / len(text4)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.457973123627309"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srfkWwm-8oVz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "477d9458-0a10-4214-83bf-3968ed2b6f34"
      },
      "source": [
        "text5.count(\"lol\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "704"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0G2ITzn8thz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0a1dd12-0610-4e53-c9d1-36737b769403"
      },
      "source": [
        "100*text5.count('lol')/len(text5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5640968673628082"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQhA_33N-HMI",
        "colab_type": "text"
      },
      "source": [
        "Instead, you can come up with your own name for a task, like \"lexical_diversity\" or \"percentage\", and associate it with a block of code. Now you only have to type a short name instead of one or more complete lines of Python code, and you can re-use it as often as you like. The block of code that does a task for us is called a function, and we define a short name for our function with the keyword def. The next example shows how to define two new functions, lexical_diversity() and percentage():\n",
        "intinya jika kita ingin membuat rumus itu jd bisa digunakan ulang tanpa membuatnya kembali kita buat saja fungsi yg me return rumus tsb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lquLIEHH86ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lexical_diversity(text):\n",
        "  return len(set(text)) / len(text)\n",
        "\n",
        "def percentage(count, total):\n",
        "  return 100*count/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6CnnoFN-xyK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "70f3f2c3-f952-41ce-8b5f-3ee9b1a2e784"
      },
      "source": [
        "print(lexical_diversity(text3))\n",
        "print(lexical_diversity(text5))\n",
        "print(percentage(4, 5))\n",
        "print(percentage(text4.count('a'), len(text4)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06230453042623537\n",
            "0.13477005109975562\n",
            "80.0\n",
            "1.457973123627309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V33t3sGKBj_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "036632e3-4b98-4cdf-ecbd-17f5d8ebc213"
      },
      "source": [
        "from nltk.book import *\n",
        "sent2"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'family',\n",
              " 'of',\n",
              " 'Dashwood',\n",
              " 'had',\n",
              " 'long',\n",
              " 'been',\n",
              " 'settled',\n",
              " 'in',\n",
              " 'Sussex',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgvRWYjzDz5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39d88990-7432-4083-c923-c5bac245c8b4"
      },
      "source": [
        "['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail']"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auAeSHp3EhRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1d5b08c8-f614-4142-e821-57e9466e9d7a"
      },
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']\n",
        "sent4 + sent1"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fellow',\n",
              " '-',\n",
              " 'Citizens',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Senate',\n",
              " 'and',\n",
              " 'of',\n",
              " 'the',\n",
              " 'House',\n",
              " 'of',\n",
              " 'Representatives',\n",
              " ':',\n",
              " 'Call',\n",
              " 'me',\n",
              " 'Ishmael',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63nqrI5zEp0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "844c95a5-ddc4-4c7a-ab91-24be62081358"
      },
      "source": [
        "sent1.append('tambah')\n",
        "print(sent1)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Call', 'me', 'Ishmael', '.', 'tambah', 'tambah', 'tambah', 'tambah']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC7NQGBsGZkW",
        "colab_type": "text"
      },
      "source": [
        "# indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaooeTd8GbH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "594a64ce-b039-49d3-caa8-8812dc067998"
      },
      "source": [
        "print(text4[173])\n",
        "print(text4.index('awaken'))\n",
        "print(text5[1:5])\n",
        "sent = ['word1', 'word2', 'word3', 'word4', 'word5','word6', 'word7', 'word8', 'word9', 'word10']\n",
        "print(sent[:3])\n",
        "print(sent[3:])\n",
        "kat = ['word1', 'word2', 'word3', 'word4', 'word5','word6', 'word7', 'word8', 'word9', 'word10']\n",
        "kat[0] = 'first'\n",
        "kat[9] = 'last'\n",
        "print(len(kat))\n",
        "print(kat)\n",
        "kat[1:9]=['second','third']\n",
        "print(kat)\n",
        "print(len(kat))\n",
        "my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode','forth', 'from', 'Camelot', '.']\n",
        "frase = my_sent[1:5]\n",
        "print(frase)\n",
        "kata = sorted(frase)\n",
        "# Remember that capitalized words appear before lowercase words in sorted lists.\n",
        "print(kata)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "awaken\n",
            "173\n",
            "['im', 'left', 'with', 'this']\n",
            "['word1', 'word2', 'word3']\n",
            "['word4', 'word5', 'word6', 'word7', 'word8', 'word9', 'word10']\n",
            "10\n",
            "['first', 'word2', 'word3', 'word4', 'word5', 'word6', 'word7', 'word8', 'word9', 'last']\n",
            "['first', 'second', 'third', 'last']\n",
            "4\n",
            "['bold', 'Sir', 'Robin', ',']\n",
            "[',', 'Robin', 'Sir', 'bold']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N31jpOgrUXqn",
        "colab_type": "text"
      },
      "source": [
        "string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT-yYMMNUY9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9c992ee4-2a78-48ff-e83c-1cd269dca3a8"
      },
      "source": [
        "name = 'Monty'\n",
        "print(name[0])\n",
        "print(name+'$')\n",
        "# join word of a list to make a single string / split a string into a list\n",
        "print('+'.join(['Monty','Python']))\n",
        "print('Monty Python'.split())"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M\n",
            "Monty$\n",
            "Monty+Python\n",
            "['Monty', 'Python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoNghKYZVKbD",
        "colab_type": "text"
      },
      "source": [
        "In this section we pick up the question of what makes a text distinct, and use automatic methods to find characteristic words and expressions of a text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ7EiBQHVN5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "95c2b881-078e-4aab-f18c-b009bc2675f6"
      },
      "source": [
        "saying = ['After', 'all', 'is', 'said', 'and', 'done', 'more', 'is', 'said', 'than', 'done']\n",
        "tokens = set(saying)\n",
        "print(tokens)\n",
        "tokens = sorted(tokens)\n",
        "print(tokens)\n",
        "print(tokens[-2:])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'After', 'than', 'done', 'is', 'and', 'more', 'all', 'said'}\n",
            "['After', 'all', 'and', 'done', 'is', 'more', 'said', 'than']\n",
            "['said', 'than']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_zEVeZUVwl6",
        "colab_type": "text"
      },
      "source": [
        "a frequency distribution, and it tells us the frequency of each vocabulary item in the text. (In general, it could count any kind of observable event.) It is a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnuWfXS1Vx7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71159834-9266-4cbf-f053-7df631c957aa"
      },
      "source": [
        "fdist1 = FreqDist(text1)\n",
        "print(fdist1)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FreqDist with 19317 samples and 260819 outcomes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPNmZvqUV_or",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAu2P3sqWADx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "0d5bff7e-0220-4f8c-fe83-547b771a488e"
      },
      "source": [
        "fdist1.most_common(50)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 18713),\n",
              " ('the', 13721),\n",
              " ('.', 6862),\n",
              " ('of', 6536),\n",
              " ('and', 6024),\n",
              " ('a', 4569),\n",
              " ('to', 4542),\n",
              " (';', 4072),\n",
              " ('in', 3916),\n",
              " ('that', 2982),\n",
              " (\"'\", 2684),\n",
              " ('-', 2552),\n",
              " ('his', 2459),\n",
              " ('it', 2209),\n",
              " ('I', 2124),\n",
              " ('s', 1739),\n",
              " ('is', 1695),\n",
              " ('he', 1661),\n",
              " ('with', 1659),\n",
              " ('was', 1632),\n",
              " ('as', 1620),\n",
              " ('\"', 1478),\n",
              " ('all', 1462),\n",
              " ('for', 1414),\n",
              " ('this', 1280),\n",
              " ('!', 1269),\n",
              " ('at', 1231),\n",
              " ('by', 1137),\n",
              " ('but', 1113),\n",
              " ('not', 1103),\n",
              " ('--', 1070),\n",
              " ('him', 1058),\n",
              " ('from', 1052),\n",
              " ('be', 1030),\n",
              " ('on', 1005),\n",
              " ('so', 918),\n",
              " ('whale', 906),\n",
              " ('one', 889),\n",
              " ('you', 841),\n",
              " ('had', 767),\n",
              " ('have', 760),\n",
              " ('there', 715),\n",
              " ('But', 705),\n",
              " ('or', 697),\n",
              " ('were', 680),\n",
              " ('now', 646),\n",
              " ('which', 640),\n",
              " ('?', 637),\n",
              " ('me', 627),\n",
              " ('like', 624)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK4Ml5S8WOqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0542832-1853-4b54-da2d-ddbf693336ee"
      },
      "source": [
        "fdist1['whale']"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    }
  ]
}